Argus YES/NO questions dataset
==============================

This dataset is meant to support answering yes/no questions based on newspaper
snippets:

	https://github.com/AugurProject/argus

The newspaper snippets are extraced by Solr fulltext search on a custom-built
news articles database based on question keywords.  Thus, for each such
snippet, both relevancy and logic relation must be jointly determined.  There
is no direct supervision for relevancy right now; each tuple is labelled by
just whether the question is true, regardless of the supporting snippet
entailing it.

Dataset contains triplets of:
  * questions: dataset of yes/no questions, partially from mTurk,
    partially auto-generated
  * answers: gold standard answer for each question
  * sentences: sentences from our Argus news-articles
    database (The Guardian, NYtimes, and a couple of archive.org-fetched RSS
    streams from ABCnews, Reuters, etc)

Data split to train-val-test sets, same as the split used for Argus evaluation.
The csv files are generated by ``tests/feature_prints/dataset.py`` in Argus.

The ``_aux`` files contain auxiliary features for each question, sentence pair
based on handcrafted feature engineering done in the Argus project; e.g.
sentiment information, subject-object sentence matches, verb wordnet matches,
etc.  Taken from ``tests/feature_prints/train/all_features.tsv`` in Argus.
Enable its usage with the ``aux_c=True aux_r=True`` options.


| Model                    | trn QAcc | val QAcc | val QF1  | tst QAcc | tst QF1   | settings
|--------------------------|----------|----------|----------|----------|-----------|----------
| avg                      | 0.871582 | 0.816243 | 0.715536 | 0.743671 | 0.671109  | (defaults)
|                          |±0.008774 |±0.007793 |±0.013344 |±0.019701 |±0.031045  |
| DAN                      | 0.883804 | 0.821856 | 0.745652 | 0.754351 | 0.691760  | ``inp_e_dropout=0`` ``inp_w_dropout=1/3`` ``deep=2`` ``pact='relu'``
|                          |±0.012438 |±0.011024 |±0.022104 |±0.025300 |±0.042076  |
|--------------------------|----------|----------|----------|----------|-----------|----------
| rnn                      | 0.906453 | 0.875000 | 0.812159 | 0.822785 | 0.781521  | ``inp_e_dropout=1/3`` ``dropout=1/3``
|                          |±0.012775 |±0.005396 |±0.008444 |±0.008261 |±0.011528  |
| cnn                      | 0.896200 | 0.856662 | 0.802337 | 0.821598 | 0.793560  | ``inp_e_dropout=1/3`` ``dropout=1/3``
|                          |±0.018262 |±0.005804 |±0.006581 |±0.006662 |±0.006533  |
| rnncnn                   | 0.884963 | 0.860030 | 0.802535 | 0.816456 | 0.780175  | ``inp_e_dropout=1/3`` ``dropout=1/3``
|                          |±0.009792 |±0.006566 |±0.007993 |±0.009081 |±0.012383  |
| attn1511                 | 0.935063 | 0.877046 | 0.821878 | 0.816456 | 0.764327  | ``focus_act='sigmoid/maxnorm'`` ``cnnact='relu'``
|                          |±0.021065 |±0.007928 |±0.012047 |±0.007572 |±0.012779  |
| Ubu. rnn w/MLP           | 0.950765 | 0.912425 | 0.865724 | 0.852057 | 0.804517  | ``vocabt='ubuntu'`` ``pdim=1`` ``ptscorer=B.mlp_ptscorer`` ``dropout=0`` ``inp_e_dropout=0`` ``task1_conf={'ptscorer':B.dot_ptscorer, 'f_add_kw':False}`` ``opt='rmsprop'``
|                          |±0.016681 |±0.004351 |±0.007591 |±0.008250 |±0.015434  |

These results are obtained like this:

	tools/train.py avg hypev data/hypev/argus/argus_train.csv data/hypev/argus/argus_val.csv
	tools/eval.py avg hypev data/hypev/argus/argus_train.csv data/hypev/argus/argus_val.csv data/hypev/argus/argus_test.csv weights-hypev-avg-69b40732de2a1d70-0*
